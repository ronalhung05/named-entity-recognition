{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of bilstm-cnn-crf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl8z5UhZHKIZ"
      },
      "source": [
        "# End to End Sequence Labelling using BiLSTM CNN CRF for NER\n",
        "This project aims to perform **End to End Sequence Labelling**  on English data from CoNLL using BiLSTM CNN CRF for Named Entity Recognition.\n",
        "\n",
        "This project aims to implement the pytorch [model](https://https://github.com/jayavardhanr/End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/blob/master/Named_Entity_Recognition-LSTM-CNN-CRF-Tutorial.ipynb) of BiLSTM CNN CRF in Tensorflow Keras. We will use Convolution Neural Network Encoding for Character Level Representation of words, Bidirectional LSTM for Word Level Encoding and Conditional Random Fields (CRF Layer) for output decodings.\n",
        "\n",
        "Following are the libraries that we will import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ygiiFRtXUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c2be40-1261-416a-d02b-60b17610292b"
      },
      "source": [
        "# Import Libraries\n",
        "# !pip install tensorflow-gpu\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "# !pip install sklearn-crfsuite\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D,Flatten, concatenate\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras_contrib.layers.crf import CRF\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import re\n",
        "import pickle\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-lh9thwq_\n",
            "  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-lh9thwq_\n",
            "  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_contrib==2.0.8) (3.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras->keras_contrib==2.0.8) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras_contrib==2.0.8) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_contrib==2.0.8) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_contrib==2.0.8) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_contrib==2.0.8) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKMIQG6IPRtC"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUikgUUDtu3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9012f7-d812-4fef-8f4e-eea119aa98ac"
      },
      "source": [
        "# Downloading Data\n",
        "!mkdir data\n",
        "!wget https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/train.txt -P /content/data\n",
        "!wget https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/dev.txt -P /content/data\n",
        "!wget https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/test.txt -P /content/data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2025-03-21 12:39:33--  https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3283420 (3.1M) [text/plain]\n",
            "Saving to: ‘/content/data/train.txt.1’\n",
            "\n",
            "train.txt.1         100%[===================>]   3.13M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-21 12:39:34 (57.6 MB/s) - ‘/content/data/train.txt.1’ saved [3283420/3283420]\n",
            "\n",
            "--2025-03-21 12:39:34--  https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827443 (808K) [text/plain]\n",
            "Saving to: ‘/content/data/dev.txt.1’\n",
            "\n",
            "dev.txt.1           100%[===================>] 808.05K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-03-21 12:39:34 (25.4 MB/s) - ‘/content/data/dev.txt.1’ saved [827443/827443]\n",
            "\n",
            "--2025-03-21 12:39:34--  https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748095 (731K) [text/plain]\n",
            "Saving to: ‘/content/data/test.txt.1’\n",
            "\n",
            "test.txt.1          100%[===================>] 730.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-03-21 12:39:34 (23.6 MB/s) - ‘/content/data/test.txt.1’ saved [748095/748095]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSdNiKA1PZCx"
      },
      "source": [
        "## Data Preprocessing\n",
        "Data prepocessing includes loading the data, updating the tagging scheme, create mapping for words, characters and tags and finally preparing the data that is passed into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnIllKuTP6zf"
      },
      "source": [
        "### Custom Data Loading\n",
        "This step includes loading the train and validation data into list of sentences.\n",
        "\n",
        "Following code loads the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDTKZWvKg5a9"
      },
      "source": [
        "def load_sentences(filename):\n",
        "    f = open(filename)\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0], splits[-1]])\n",
        "\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    return sentences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3XiPkqAhaAv",
        "outputId": "35138083-2082-488b-fab7-96026a58f5aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_sentences = load_sentences(\"/content/data/train.txt\")\n",
        "dev_sentences = load_sentences(\"/content/data/dev.txt\")\n",
        "test_sentences = load_sentences(\"/content/data/test.txt\")\n",
        "len(train_sentences), len(dev_sentences), len(test_sentences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 3250, 3453)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWEgS4CLkVrI"
      },
      "source": [
        "### Add Character Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i502-I7kaga"
      },
      "source": [
        "def add_char_info(sentences):\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for j, data in enumerate(sentence):\n",
        "            chars = [c for c in data[0]]\n",
        "            sentences[i][j] = [data[0], chars, data[1]]\n",
        "    return sentences"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9_LNuUzkwDM",
        "outputId": "9b10807d-0f29-426c-8f99-46ff4cea7533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_sentences = add_char_info(train_sentences)\n",
        "dev_sentences = add_char_info(dev_sentences)\n",
        "test_sentences = add_char_info(test_sentences)\n",
        "train_sentences[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['EU', ['E', 'U'], 'B-ORG\\n'],\n",
              " ['rejects', ['r', 'e', 'j', 'e', 'c', 't', 's'], 'O\\n'],\n",
              " ['German', ['G', 'e', 'r', 'm', 'a', 'n'], 'B-MISC\\n'],\n",
              " ['call', ['c', 'a', 'l', 'l'], 'O\\n'],\n",
              " ['to', ['t', 'o'], 'O\\n'],\n",
              " ['boycott', ['b', 'o', 'y', 'c', 'o', 't', 't'], 'O\\n'],\n",
              " ['British', ['B', 'r', 'i', 't', 'i', 's', 'h'], 'B-MISC\\n'],\n",
              " ['lamb', ['l', 'a', 'm', 'b'], 'O\\n'],\n",
              " ['.', ['.'], 'O\\n']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhgvvGM0c1wx"
      },
      "source": [
        "### Tag Mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG6_AoaYns4M"
      },
      "source": [
        "labels_set = set()\n",
        "words = {}\n",
        "\n",
        "# unique words and labels in data\n",
        "for dataset in [train_sentences, dev_sentences, test_sentences]:\n",
        "  for sentence in dataset:\n",
        "    for token, char, label in sentence:\n",
        "      # token ... token, char ... list of chars, label ... BIO labels\n",
        "      labels_set.add(label)\n",
        "      words[token.lower()] = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bYDV3BawL7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11fdbe29-6253-478a-8874-23d74ed548d8"
      },
      "source": [
        "# mapping for labels\n",
        "indexes = {\"PADDING\":0}\n",
        "for label in labels_set:\n",
        "  indexes[label] = len(indexes)\n",
        "\n",
        "\n",
        "tag_to_id = {}\n",
        "for word,index in indexes.items():\n",
        "  if index != 0:\n",
        "    word = word[:len(word)-1]\n",
        "\n",
        "  tag_to_id[word] = index\n",
        "\n",
        "tag_to_id"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PADDING': 0,\n",
              " 'B-ORG': 1,\n",
              " 'I-MISC': 2,\n",
              " 'I-ORG': 3,\n",
              " 'I-LOC': 4,\n",
              " 'B-PER': 5,\n",
              " 'B-LOC': 6,\n",
              " 'O': 7,\n",
              " 'I-PER': 8,\n",
              " 'B-MISC': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwnGYuCWAPRf",
        "outputId": "1f90b973-6b9a-466d-8295-3b6769adb2ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
        "len(id_to_tag)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVTId7Mrdbta"
      },
      "source": [
        "### Word and Character Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jxn9lOFIlrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5caa6d8-67ab-4a6c-d760-1c48ee165302"
      },
      "source": [
        "# Download Glove Word Embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip -d /content/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-21 12:39:37--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-03-21 12:39:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-03-21 12:39:37--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2025-03-21 12:42:16 (5.18 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace /content/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/glove.6B.50d.txt  \n",
            "replace /content/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: /content/glove.6B.100d.txt  Y\n",
            "\n",
            "replace /content/glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: /content/glove.6B.200d.txt  Y\n",
            "Y\n",
            "\n",
            "replace /content/glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: /content/glove.6B.300d.txt  Y\n",
            "Y\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU6qCsopptPu",
        "outputId": "1ffe51d0-b0f1-4e89-de16-28dd52fa96d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_to_id = {}\n",
        "word_embeddings = []\n",
        "\n",
        "EMBEDDINGS_FILE = open(\"/content/glove.6B.50d.txt\", encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# loop through each word in embeddings\n",
        "for line in EMBEDDINGS_FILE:\n",
        "    split = line.strip().split(\" \")\n",
        "    word = split[0]  # embedding word entry\n",
        "\n",
        "    if len(word_to_id) == 0:  # add padding+unknown\n",
        "        word_to_id[\"PADDING_TOKEN\"] = len(word_to_id)\n",
        "        vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "        word_embeddings.append(vector)\n",
        "\n",
        "        word_to_id[\"UNKNOWN_TOKEN\"] = len(word_to_id)\n",
        "        vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
        "        word_embeddings.append(vector)\n",
        "\n",
        "    if split[0].lower() in words:\n",
        "        vector = np.array([float(num) for num in split[1:]])\n",
        "        word_embeddings.append(vector)  # word embedding vector\n",
        "        word_to_id[split[0]] = len(word_to_id)  # corresponding word dict\n",
        "\n",
        "word_embeddings = np.array(word_embeddings)\n",
        "word_embeddings.shape, len(word_to_id)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((22949, 50), 22949)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJK6yFDBhwD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfe26d7-c154-4cd7-d246-238b051a834f"
      },
      "source": [
        "id_to_word = {v: k for k, v in word_to_id.items()}\n",
        "len(id_to_word)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22949"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8z1AX6Z6H7R",
        "outputId": "f54ffefc-81d1-4cc1-b91d-1d082f3b62e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# dictionary of all possible characters\n",
        "char_to_id = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "    char_to_id[c] = len(char_to_id)\n",
        "\n",
        "len(char_to_id)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYdml-AoeDDJ"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pyg5UOr7cYw"
      },
      "source": [
        "def create_dataset(sentences, word_to_id, tag_to_id, char_to_id):\n",
        "    unk_index = word_to_id['UNKNOWN_TOKEN']\n",
        "    pad_index = word_to_id['PADDING_TOKEN']\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    word_count = 0\n",
        "    unk_word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_indices = []\n",
        "        char_indices = []\n",
        "        tag_indices = []\n",
        "\n",
        "        for word, char, tag in sentence:\n",
        "            word_count += 1\n",
        "            if word in word_to_id:\n",
        "                word_index = word_to_id[word]\n",
        "            elif word.lower() in word_to_id:\n",
        "                word_index = word_to_id[word.lower()]\n",
        "            else:\n",
        "                word_index = unk_index\n",
        "                unk_word_count += 1\n",
        "            char_index = []\n",
        "            for x in char:\n",
        "                char_index.append(char_to_id[x])\n",
        "            # Get the label and map to int\n",
        "            word_indices.append(word_index)\n",
        "            char_indices.append(char_index)\n",
        "            tag_indices.append(tag_to_id[tag])\n",
        "\n",
        "        dataset.append([word_indices, char_indices, tag_indices])\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fr7rQo796oZ"
      },
      "source": [
        "def padding(sentences):\n",
        "  maxlen = 52\n",
        "  for sentence in sentences:\n",
        "      char = sentence[1]\n",
        "      for x in char:\n",
        "          maxlen = max(maxlen, len(x))\n",
        "  for i, sentence in enumerate(sentences):\n",
        "      sentences[i][1] = keras.preprocessing.sequence.pad_sequences(sentences[i][1], 52, padding='post')\n",
        "  return sentences"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAspZV_I-blR"
      },
      "source": [
        "train_set = padding(create_dataset(train_sentences, word_to_id, indexes, char_to_id))\n",
        "dev_set = padding(create_dataset(dev_sentences, word_to_id, indexes, char_to_id))\n",
        "test_set = padding(create_dataset(test_sentences, word_to_id, indexes, char_to_id))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o04Uf_HQFbfk"
      },
      "source": [
        "def unpack(dataset):\n",
        "  words=[]\n",
        "  chars=[]\n",
        "  tags = []\n",
        "  for word, char, tag in dataset:\n",
        "    words.append(word)\n",
        "    chars.append(char)\n",
        "    tags.append(tag)\n",
        "\n",
        "  words = keras.preprocessing.sequence.pad_sequences(words, 52, padding='post')\n",
        "  chars = keras.preprocessing.sequence.pad_sequences(chars, 52, padding='post')\n",
        "  tags = keras.preprocessing.sequence.pad_sequences(tags, 52, padding='post')\n",
        "  tags = keras.utils.to_categorical(tags, num_classes=10)\n",
        "\n",
        "  return words, chars, tags"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEISynBrHT6H"
      },
      "source": [
        "train_words, train_chars, train_tags = unpack(train_set)\n",
        "valid_words, valid_chars, valid_tags = unpack(dev_set)\n",
        "test_words, test_chars, test_tags = unpack(test_set)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJN7CltCStrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9717a50f-9af8-4233-e49b-86b1165a9208"
      },
      "source": [
        "train_words.shape, train_chars.shape, train_tags.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14041, 52), (14041, 52, 52), (14041, 52, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr77G9SSdmam"
      },
      "source": [
        "## Define Model\n",
        "The model that is implemented in this project uses the following architectures:\n",
        "- Convolution Neural Network for Character Level Representation of words\n",
        "- Bidirectional LSTM for Word Level Encoding\n",
        "- Conditional Random Fields (CRF Layer) for output decodings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import constraints, initializers, regularizers\n",
        "\n",
        "class CRF(Layer):\n",
        "    \"\"\"Conditional Random Field layer.\n",
        "    Implementation of CRF layer with similar API to keras-contrib's CRF.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units=None, sparse_target=False,\n",
        "                 learn_mode='join', test_mode='viterbi',\n",
        "                 use_boundary=True, use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 chain_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 boundary_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 chain_regularizer=None,\n",
        "                 boundary_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 chain_constraint=None,\n",
        "                 boundary_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(CRF, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.sparse_target = sparse_target\n",
        "        self.learn_mode = learn_mode\n",
        "        self.test_mode = test_mode\n",
        "        self.use_boundary = use_boundary\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.chain_initializer = initializers.get(chain_initializer)\n",
        "        self.boundary_initializer = initializers.get(boundary_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.chain_regularizer = regularizers.get(chain_regularizer)\n",
        "        self.boundary_regularizer = regularizers.get(boundary_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.chain_constraint = constraints.get(chain_constraint)\n",
        "        self.boundary_constraint = constraints.get(boundary_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_dim = input_shape[-1]\n",
        "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: self.input_dim})\n",
        "\n",
        "        if self.units is None:\n",
        "            self.units = self.input_dim\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(self.input_dim, self.units),\n",
        "            name='kernel',\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint\n",
        "        )\n",
        "\n",
        "        self.chain_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units),\n",
        "            name='chain_kernel',\n",
        "            initializer=self.chain_initializer,\n",
        "            regularizer=self.chain_regularizer,\n",
        "            constraint=self.chain_constraint\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(self.units,),\n",
        "                name='bias',\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint\n",
        "            )\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        if self.use_boundary:\n",
        "            self.left_boundary = self.add_weight(\n",
        "                shape=(self.units,),\n",
        "                name='left_boundary',\n",
        "                initializer=self.boundary_initializer,\n",
        "                regularizer=self.boundary_regularizer,\n",
        "                constraint=self.boundary_constraint\n",
        "            )\n",
        "            self.right_boundary = self.add_weight(\n",
        "                shape=(self.units,),\n",
        "                name='right_boundary',\n",
        "                initializer=self.boundary_initializer,\n",
        "                regularizer=self.boundary_regularizer,\n",
        "                constraint=self.boundary_constraint\n",
        "            )\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, mask=None, training=None):\n",
        "        potentials = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            potentials = potentials + self.bias\n",
        "\n",
        "        if mask is None:\n",
        "            mask = tf.ones_like(inputs[:, :, 0], dtype=tf.bool)\n",
        "        else:\n",
        "            mask = tf.cast(mask, dtype=tf.bool)\n",
        "\n",
        "        sequence_lengths = tf.reduce_sum(tf.cast(mask, tf.int64), axis=1)\n",
        "\n",
        "        if training:\n",
        "            return potentials\n",
        "        else:\n",
        "            viterbi_sequence, _ = self.viterbi_decode(potentials, sequence_lengths)\n",
        "            return tf.one_hot(viterbi_sequence, self.units)\n",
        "\n",
        "    def loss_function(self, y_true, y_pred):\n",
        "        if self.sparse_target:\n",
        "            y_true = tf.one_hot(tf.cast(y_true, tf.int32), self.units)\n",
        "\n",
        "        log_likelihood, _ = self.forward_algorithm(y_pred, y_true)\n",
        "        return -log_likelihood\n",
        "\n",
        "    def viterbi_decode(self, potentials, sequence_lengths):\n",
        "        # Implementation of Viterbi decoding\n",
        "        # This is a simplified version - in a real implementation, you would use tf.TensorArray\n",
        "        # for more efficient dynamic computation\n",
        "\n",
        "        batch_size = tf.shape(potentials)[0]\n",
        "        max_seq_len = tf.shape(potentials)[1]\n",
        "\n",
        "        # Initialize with left boundary if used\n",
        "        if self.use_boundary:\n",
        "            initial_state = self.left_boundary\n",
        "        else:\n",
        "            initial_state = tf.zeros([self.units], dtype=potentials.dtype)\n",
        "\n",
        "        # Create a mask for valid positions\n",
        "        mask = tf.sequence_mask(sequence_lengths, maxlen=max_seq_len)\n",
        "\n",
        "        def _viterbi_step(previous, current):\n",
        "            emissions = current[0]\n",
        "            mask_t = current[1]\n",
        "\n",
        "            previous = tf.expand_dims(previous, 2)  # (batch, num_tags, 1)\n",
        "            transition_scores = tf.expand_dims(self.chain_kernel, 0)  # (1, num_tags, num_tags)\n",
        "\n",
        "            # Calculate scores for all possible paths\n",
        "            scores = previous + transition_scores\n",
        "\n",
        "            # Find the best path\n",
        "            best_scores = tf.reduce_max(scores, axis=1)\n",
        "            best_paths = tf.argmax(scores, axis=1)\n",
        "\n",
        "            # Add emission scores\n",
        "            scores_with_emissions = best_scores + emissions\n",
        "\n",
        "            # Apply mask\n",
        "            mask_t = tf.expand_dims(mask_t, 1)\n",
        "            scores_masked = scores_with_emissions * tf.cast(mask_t, scores_with_emissions.dtype)\n",
        "\n",
        "            return scores_masked, best_paths\n",
        "\n",
        "        # Iterate through the sequence\n",
        "        initial_scores = initial_state + potentials[:, 0]\n",
        "\n",
        "        # Placeholder for the implementation\n",
        "        # In a full implementation, you would use tf.scan or a loop to compute the Viterbi path\n",
        "\n",
        "        # For now, return a dummy implementation\n",
        "        best_paths = tf.argmax(potentials, axis=-1)\n",
        "        return best_paths, None\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.units,\n",
        "            'sparse_target': self.sparse_target,\n",
        "            'learn_mode': self.learn_mode,\n",
        "            'test_mode': self.test_mode,\n",
        "            'use_boundary': self.use_boundary,\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'chain_initializer': initializers.serialize(self.chain_initializer),\n",
        "            'boundary_initializer': initializers.serialize(self.boundary_initializer),\n",
        "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'chain_regularizer': regularizers.serialize(self.chain_regularizer),\n",
        "            'boundary_regularizer': regularizers.serialize(self.boundary_regularizer),\n",
        "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "            'chain_constraint': constraints.serialize(self.chain_constraint),\n",
        "            'boundary_constraint': constraints.serialize(self.boundary_constraint),\n",
        "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
        "        }\n",
        "        base_config = super(CRF, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "    def forward_algorithm(self, potentials, y_true):\n",
        "        # Triển khai thuật toán forward để tính log-likelihood\n",
        "        # Đây là phần giả lập, cần triển khai đầy đủ\n",
        "        sequence_lengths = tf.reduce_sum(tf.cast(tf.not_equal(tf.argmax(y_true, axis=-1), 0), tf.int32), axis=-1)\n",
        "        log_likelihood = tf.reduce_sum(potentials * y_true, axis=[1, 2])\n",
        "        return log_likelihood, sequence_lengths\n",
        "\n",
        "\n",
        "def crf_loss(y_true, y_pred):\n",
        "    \"\"\"CRF loss function.\n",
        "\n",
        "    Args:\n",
        "        y_true: True target tensor.\n",
        "        y_pred: Predicted tensor from CRF layer.\n",
        "\n",
        "    Returns:\n",
        "        Negative log-likelihood loss.\n",
        "    \"\"\"\n",
        "    # Use tf.cond for conditional execution in graph mode\n",
        "    y_true = tf.cond(tf.equal(tf.rank(y_true), 2),\n",
        "                     lambda: tf.one_hot(tf.cast(y_true, tf.int32), tf.shape(y_pred)[-1]),\n",
        "                     lambda: y_true)\n",
        "\n",
        "    # Compute negative log-likelihood\n",
        "    # Đây là một placeholder cho việc triển khai thực tế\n",
        "    log_likelihood = -tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "    return tf.reduce_mean(log_likelihood)\n",
        "\n",
        "def crf_viterbi_accuracy(y_true, y_pred):\n",
        "    \"\"\"Accuracy based on Viterbi path.\n",
        "\n",
        "    Args:\n",
        "        y_true: True target tensor.\n",
        "        y_pred: Predicted tensor from CRF layer.\n",
        "\n",
        "    Returns:\n",
        "        Accuracy metric.\n",
        "    \"\"\"\n",
        "    # Convert to class indices\n",
        "    y_pred_argmax = tf.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # If y_true is one-hot, convert to indices\n",
        "    # Instead of `len(tf.shape(y_true))`, use `tf.shape(y_true).shape[0]`\n",
        "    if tf.shape(y_true).shape[0] == 3:\n",
        "        y_true = tf.argmax(y_true, axis=-1)\n",
        "\n",
        "    # Create mask for valid positions (non-padding)\n",
        "    mask = tf.not_equal(y_true, 0)\n",
        "\n",
        "    # Calculate accuracy only on valid positions\n",
        "    correct = tf.cast(tf.equal(y_true, y_pred_argmax), tf.float32) * tf.cast(mask, tf.float32)\n",
        "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(tf.cast(mask, tf.float32))\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "YTF3vnyew-Az"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWwh_t3DdlEy",
        "outputId": "c902e20d-3227-4f71-f01f-61f2929e371f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "# Define Model Layers\n",
        "char_input = Input(shape=(None, 52,), name=\"char_input\")\n",
        "char_embed = TimeDistributed(Embedding(len(char_to_id), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"char_embed\")(char_input)\n",
        "char_dropout = Dropout(0.5)(char_embed)\n",
        "char_cnn = TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same', activation='tanh', strides=1), name=\"conv1d\")(char_dropout)\n",
        "maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"maxpool\")(char_cnn)\n",
        "char_flat = TimeDistributed(Flatten(), name=\"flatten\")(maxpool_out)\n",
        "char = Dropout(0.5)(char_flat)\n",
        "words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "words = Embedding(input_dim=word_embeddings.shape[0], output_dim=word_embeddings.shape[1], weights=[word_embeddings],trainable=False)(words_input)\n",
        "concat = concatenate([words, char])\n",
        "lstm = Bidirectional(LSTM(200, return_sequences=True, dropout=0.5,recurrent_dropout=0.25), name=\"bilstm\")(concat)\n",
        "dense_out = TimeDistributed(Dense(len(tag_to_id)), name=\"dense_layer\")(lstm)\n",
        "\n",
        "# Sử dụng lớp CRF tùy chỉnh\n",
        "crf_layer = CRF(len(tag_to_id))\n",
        "output = crf_layer(dense_out)\n",
        "\n",
        "# Define Model Inputs and Output\n",
        "model = keras.models.Model([char_input, words_input], output)\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
        "# Model Summary\n",
        "model.summary()\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_16\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_16\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ char_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ char_embed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │          \u001b[38;5;34m2,910\u001b[0m │ char_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_54 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ char_embed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mTimeDistributed\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │          \u001b[38;5;34m2,730\u001b[0m │ dropout_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ maxpool (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ words_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ maxpool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_55 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │      \u001b[38;5;34m1,147,450\u001b[0m │ words_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_55 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_27            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ dropout_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bilstm (\u001b[38;5;33mBidirectional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)      │        \u001b[38;5;34m449,600\u001b[0m │ concatenate_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_layer               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)       │          \u001b[38;5;34m4,010\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ crf_22 (\u001b[38;5;33mCRF\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)       │            \u001b[38;5;34m230\u001b[0m │ dense_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ char_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ char_embed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,910</span> │ char_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ char_embed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,730</span> │ dropout_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ maxpool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ words_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ maxpool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,147,450</span> │ words_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_27            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ dropout_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bilstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">449,600</span> │ concatenate_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_layer               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,010</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ crf_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CRF</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">230</span> │ dense_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,606,930\u001b[0m (6.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,930</span> (6.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m459,480\u001b[0m (1.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">459,480</span> (1.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,147,450\u001b[0m (4.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,147,450</span> (4.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbvFpoZGeEhT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvHdcsVSV4Un",
        "outputId": "77f6fecf-b1e4-4630-fd39-795bc3e47667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "history = model.fit([train_chars,train_words], train_tags,validation_data=[[valid_chars, valid_words], valid_tags],batch_size=64,epochs=20,verbose=1)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['crf_22/chain_kernel', 'crf_22/left_boundary', 'crf_22/right_boundary'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 464ms/step - crf_viterbi_accuracy: 0.0029 - loss: -92.0694 - val_crf_viterbi_accuracy: 0.0000e+00 - val_loss: -0.6980\n",
            "Epoch 2/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 433ms/step - crf_viterbi_accuracy: 0.0000e+00 - loss: -915.6615 - val_crf_viterbi_accuracy: 0.0000e+00 - val_loss: -0.6980\n",
            "Epoch 3/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 441ms/step - crf_viterbi_accuracy: 0.0000e+00 - loss: -2696.1038 - val_crf_viterbi_accuracy: 0.0000e+00 - val_loss: -0.6980\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-058b4f88b952>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_chars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov5oZ-Hu9Ue4"
      },
      "source": [
        "## Training and Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASdrbbrj5l9n",
        "outputId": "02c2d684-2acf-40e4-de78-58d417cbb338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "_, train_acc = model.evaluate([train_chars, train_words],train_tags)\n",
        "_, val_acc = model.evaluate([valid_chars, valid_words], valid_tags)\n",
        "_, test_acc = model.evaluate([test_chars, test_words],test_tags)\n",
        "\n",
        "print('Training Accuray: ', train_acc * 100)\n",
        "print('Validation Accuray: ', val_acc * 100)\n",
        "print('Testing Accuray: ', test_acc * 100)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Exception encountered when calling CRF.call().\n\n\u001b[1mmodule 'tensorflow' has no attribute 'contrib'\u001b[0m\n\nArguments received by CRF.call():\n  • inputs=tf.Tensor(shape=(None, 52, 10), dtype=float32)\n  • mask=None\n  • training=False",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-75f7cc984125>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Accuray: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-b0d913db543a>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Decode the highest scoring sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             viterbi_sequence, _ = tf.contrib.crf.crf_decode(\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling CRF.call().\n\n\u001b[1mmodule 'tensorflow' has no attribute 'contrib'\u001b[0m\n\nArguments received by CRF.call():\n  • inputs=tf.Tensor(shape=(None, 52, 10), dtype=float32)\n  • mask=None\n  • training=False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiP8C1w_MiFN"
      },
      "source": [
        "## Evaluation Metrics Report\n",
        "Now we will use Precision, Recall and F1 score to evaluate the performance of our model on each tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJUHJ1cUN-2D"
      },
      "source": [
        "# Evaluate\n",
        "pred_cat = model.predict([test_chars,test_words])\n",
        "predicted = [[[np.argmax(i)] for i in w] for w in pred_cat]\n",
        "predicted = np.array(predicted)\n",
        "actual = [[[np.argmax(i)] for i in w] for w in test_tags]\n",
        "actual = np.array(actual)\n",
        "# Convert the index to tag\n",
        "predicted_tag = [[id_to_tag[i[0]] for i in row] for row in predicted]\n",
        "actual_tag = [[id_to_tag[i[0]] for i in row] for row in actual]\n",
        "\n",
        "# Metrics Report\n",
        "report = flat_classification_report(y_pred=predicted_tag, y_true=actual_tag)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_UKGpvbglJ9"
      },
      "source": [
        "## Accuracy Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMT0KbXguC4"
      },
      "source": [
        "# Plotting Training vs Validation Accuracy\n",
        "plt.plot(history.history['crf_viterbi_accuracy'])\n",
        "plt.plot(history.history['val_crf_viterbi_accuracy'])\n",
        "plt.title('Train vs Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD2nkcvOhGrU"
      },
      "source": [
        "## Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3VmoyT0hGrb"
      },
      "source": [
        "# Plotting Training vs Validation Accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Train vs Validation Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4hAqgKiopiE"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf0DniOFpCu7"
      },
      "source": [
        "good_example = []\n",
        "bad_example = []\n",
        "\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    if test_words[i][j] != 0:\n",
        "      word = id_to_word[test_words[i][j]]\n",
        "      a_tag = actual_tag[i][j]\n",
        "      p_tag = predicted_tag[i][j]\n",
        "      if a_tag == p_tag:\n",
        "        good_example.append([word, p_tag, a_tag])\n",
        "      else:\n",
        "        bad_example.append([word, p_tag, a_tag])\n",
        "\n",
        "col1_width = max([len(x[0]) for x in good_example])\n",
        "col2_width = max([len(x[1]) for x in good_example])\n",
        "col3_width = max([len(x[2]) for x in good_example])\n",
        "\n",
        "print(\"-------- Good Examples --------\")\n",
        "print (\"|{0:<{col1}}  |{1:<{col2}}  |{2:<{col3}}  |\".format(\"Word\",\"Actual\",\"Predicted\",col1=col1_width,col2=col2_width,col3=col3_width))\n",
        "\n",
        "for word, p_tag, a_tag in good_example:\n",
        "  print (\"|{0:<{col1}}  |{1:<{col2}}   |{2:<{col3}}      |\".format(word,a_tag,p_tag,col1=col1_width,col2=col2_width,col3=col3_width))\n",
        "\n",
        "col1_width = max([len(x[0]) for x in bad_example])\n",
        "col2_width = max([len(x[1]) for x in bad_example])\n",
        "col3_width = max([len(x[2]) for x in bad_example])\n",
        "\n",
        "print(\"\\n\\n-------- Bad Examples --------\")\n",
        "print (\"|{0:<{col1}}  |{1:<{col2}}  |{2:<{col3}}  |\".format(\"Word\",\"Actual\",\"Predicted\",col1=col1_width,col2=col2_width,col3=col3_width))\n",
        "\n",
        "for word, p_tag, a_tag in bad_example:\n",
        "   print (\"|{0:<{col1}}  |{1:<{col2}}   |{2:<{col3}}      |\".format(word,a_tag,p_tag,col1=col1_width,col2=col2_width,col3=col3_width))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEDxJVTGsRKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}